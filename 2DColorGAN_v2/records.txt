v1:
v2:
v3:add in early_stop
v4:
v5:
v6:
v7:
v8:
v9:add in VGG
v10:add in 3000 character for one pair
    !!loss weight, it has been covered
v11:decrease at 1000 character added in  v10
v12:img_sketch weight at 0.7 , img_reference weight at 0.3
v13:img_reference enbedding, add class ColorEmbedding in generator_model and discriminator_model
v14:img_sketch weight at 1, img_reference weight at 1
v15:img_sketch weight at 0.5, img_reference weight at 0.5
    !!loss weight, it has been covered without training
v16:2DColorGAN2, "change generator_model" and "discriminator_model".
    DeepHist to catch the color from img_reference.
    Exchange Loss function L1,VGG to EMD
v17:img_sketch weight at 0.3, img_reference weight at 0.7
    discriminator_model 
        replace discriminator_model CNNBLOCK 
            self.norm = nn.BatchNorm2d(out_channels)
            to nn.InstanceNorm2d(out_channels)
        
    train for discriminator from 3 to 1 times
    L1_LAMBDA = 120 >> L1_LAMBDA = 50
v18:discriminator_model
        train loop from 3 to 5
        D_fake_loss = bce(D_fake, torch.ones_like(D_fake))
            to D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))
    L1_LAMBDA = 50 >> L1_LAMBDA = 0.4
    LEARNING_RATE = 2e-4 >> LEARNING_RATE = 3e-4
v19:discriminator_model
        self-attenction
            scale = (q.size(-1) ** 0.5)  # 縮放避免內積過大
                to scale = (q.size(-1) ** 0.5) + 1e-8
            attn = F.softmax(torch.bmm(q, k) / scale, dim=-1)
                to attn = F.softmax((torch.bmm(q, k) / scale).clamp(-10, 10), dim=-1)
    train
        add
            torch.nn.utils.clip_grad_norm_(disc.parameters(), max_norm=1.0)
            torch.nn.utils.clip_grad_norm_(gen.parameters(), max_norm=1.0)

            assert img_fake.min() >= 0 and img_fake.max() <= 1, "VGG 輸入範圍錯誤"
            assert not torch.isnan(emd), "EMD 計算出現 nan"
        Exchange
            train discriminator
                loop for 5 to 3
            if config.SAVE_MODEL:
                to if config.SAVE_MODEL and (G_loss < Min_loss or epoch % 5 == 0):
            if epoch % 20 == 0:
                to if epoch % 5 == 0:
    config
        Exchange
            LEARNING_RATE = 3e -4 >> LEARNING_RATE = 2e - 4
            L1_LAMBDA = 0.4 >> L1_LAMBDA = 10
            VGG_LAMBDA = 0.1 >> VGG_LAMBDA = 0.5
            EMD_LAMBDA = 0.5 >> EMD_LAMBDA = 1
    loss
        Exchange
            class VGGPerceptualLoss(nn.Module):
                def __init__(self):
                    super(VGGPerceptualLoss, self).__init__()
                    self.vgg = models.vgg19(pretrained=True).features[:32]  # 取前 32 層
                    self.vgg = self.vgg.to(config.DEVICE)
                    for param in self.vgg.parameters():
                        param.requires_grad = False  # 固定 VGG 權重

                def forward(self, x, y):
                    #  將 [-1, 1] 範圍映射到 [0, 1]
                    x = (x + 1) / 2  
                    y = (y + 1) / 2  

                    #  使用 ImageNet 標準化 (VGG 模型需求)
                    x = TF.normalize(x, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                    y = TF.normalize(y, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

                    #  提取 VGG 特徵
                    x_features = self.vgg(x.float())  
                    y_features = self.vgg(y.float())  

                    #  計算 L1 感知損失
                    return F.l1_loss(x_features, y_features)
v20:
    L1_LAMBDA = 10 >> L1_LAMBDA = 100
v21:
    train
        stop
            VGG and emd
    config
        VGG_LAMBDA = 0.5 >> VGG_LAMBDA = 0.1
        EMD_LAMBDA = 1 >> EMD_LAMBDA = 0.5
v22:
    discriminator_model
        Exchange back
            scale = (q.size(-1) ** 0.5)  # 縮放避免內積過大
                to scale = (q.size(-1) ** 0.5) + 1e-8
            attn = F.softmax(torch.bmm(q, k) / scale, dim=-1)
                to attn = F.softmax((torch.bmm(q, k) / scale).clamp(-10, 10), dim=-1)
    
 v23:
    color_embeddinng
        delete
            # 將像素值從 [-1, 1] 轉為 [0, 1]
            img_reference = (img_reference + 1) / 2  
        Exchange
             hist = torch.stack([torch.histc(channel[i], bins=self.bins, min=0, max=1.0) for i in range(batch_size)], dim=0)  # (B, bins)
                to  hist = torch.stack([torch.histc(channel[i], bins=self.bins, min=-1, max=1.0) for i in range(batch_size)], dim=0)  # (B, bins)
    discriminator_model
        Exchange
            attn = F.softmax(torch.bmm(q, k) / scale, dim=-1)
                to attn = F.softmax((torch.bmm(q, k) / scale).clamp(-10, 10), dim=-1)
v24:
    train
        discriminator for 3 to 1 times train
v25:
    train
        restart
            VGG and EBD
v26:
    config
        VGG_LAMBDA = 0.1 >> VGG_LAMBDA = 0.5
        EMD_LAMBDA = 0.5 >> EMD_LAMBDA = 1